{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "393e0d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import os\n",
    "import logging\n",
    "import pickle as pk\n",
    "from collections import defaultdict\n",
    "\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from patchify import patchify,unpatchify\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "logging.getLogger(\"PIL\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"matplotlib\").setLevel(logging.WARNING)\n",
    "\n",
    "PIL.Image.MAX_IMAGE_PIXELS = 933120000\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from skimage.measure import block_reduce\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "from torchvision.models.resnet import resnet50, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb61576c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MapPatch():\n",
    "    def __init__(self, patch, patch_index, origin_map):\n",
    "        self.patch = patch\n",
    "        self.patch_index = patch_index\n",
    "        self.origin_map = origin_map\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_map_patches(file_name, patch_width, map_transformer = None, verbose = True):\n",
    "        tif_map = PIL.Image.open(file_name)\n",
    "        tif_map_np = np.array(tif_map)\n",
    "        \n",
    "        if map_transformer is not None and verbose:\n",
    "            logging.info(f\"Applying transformation {map_transformer.__name__} to {file_name}\")\n",
    "            tif_map_np = map_transformer(tif_map_np)\n",
    "        \n",
    "        tif_map_patches = patchify(image = tif_map_np, \n",
    "                                   patch_size = (patch_width, patch_width, 3),\n",
    "                                   step = patch_width)\n",
    "\n",
    "        if verbose:\n",
    "            logging.info(f\"{np.prod(tif_map_patches.shape[:2]):,} patches from {file_name} generated with shape {tif_map_patches.shape}\")\n",
    "\n",
    "        return tif_map_np, tif_map_patches\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_map_patch_list(file_name, patch_width, map_transformer = None, verbose = True):\n",
    "        _, tif_map_patches = MapPatch.get_map_patches(file_name, \n",
    "                                                      patch_width, \n",
    "                                                      map_transformer = map_transformer, \n",
    "                                                      verbose = verbose)\n",
    "        patches = []\n",
    "        \n",
    "        for i in range(tif_map_patches.shape[0]):\n",
    "            for j in range(tif_map_patches.shape[1]):\n",
    "                patches.append(MapPatch(tif_map_patches[i,j,0], patch_index = (i,j), origin_map = file_name))\n",
    "                \n",
    "        return patches\n",
    "    \n",
    "    def show(self, verbose = True):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(self.patch)\n",
    "        \n",
    "        if verbose:\n",
    "            ax.set_title(f\"Patch at {self.patch_index} from {self.origin_map}.\")\n",
    "            \n",
    "        plt.show()\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.patch_index == other.patch_index and self.origin_map == other.origin_map and np.all(np.isclose(self.patch, other.patch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d716688a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchDataset(Dataset):\n",
    "    def __init__(self, patches):\n",
    "        self.patches = patches\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        if isinstance(i, slice):\n",
    "            start = i.start if i.start else 0\n",
    "            stop = i.stop if i.stop else len(self.patches)\n",
    "            step = i.step if i.step else 1\n",
    "            \n",
    "            return [(self.patches[j], self.patches[j].origin_map) for j in range(start, stop, step)]\n",
    "        \n",
    "        return (self.patches[i], self.patches[i].origin_map)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dir(cls, directory, file_ext, patch_width, map_transformer = None):\n",
    "        patches = []\n",
    "        \n",
    "        if file_ext == \"tif\":\n",
    "            for file in os.listdir(directory):\n",
    "                if file.endswith(\"tif\"):\n",
    "                    file_name = f\"{directory}/{file}\"\n",
    "                    logging.info(f\"Fetching patches from {file_name}.\")\n",
    "                    patches.extend(MapPatch.get_map_patch_list(file_name = file_name, \n",
    "                                                               patch_width = patch_width, \n",
    "                                                               map_transformer = map_transformer,\n",
    "                                                               verbose = True))\n",
    "        elif file_ext == \"pk\":\n",
    "            for file in os.listdir(directory):\n",
    "                if file.endswith(\"pk\"):\n",
    "                    file_name = f\"{directory}/{file}\"\n",
    "                    logging.info(f\"Fetching patches from {file_name}.\")\n",
    "                    with open(file_name, \"rb\") as f:\n",
    "                        patches.extend(pk.load(file_name))\n",
    "        else:\n",
    "            print(f\"{file_ext} is an invalid file format. Require tif or pk.\")\n",
    "            \n",
    "        return cls(patches)\n",
    "    \n",
    "    def to_pickle(self, file_name = None):\n",
    "        with open(f\"{file_name}.pk\", \"wb\") as f:\n",
    "            pk.dump(self.patches, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9507103b-28ba-45b5-938f-141039dd9e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLPatchDataset(Dataset):\n",
    "    def __init__(self, patches):\n",
    "        self.patches = patches\n",
    "        self.patch_dict = self.__get_patch_dict(self.patches)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        if isinstance(i, slice):\n",
    "            start = i.start if i.start else 0\n",
    "            stop = i.stop if i.stop else len(self.patches)\n",
    "            step = i.step if i.step else 1\n",
    "            \n",
    "            return [(self.patches[j], self.__get_matching_patches(self.patches[j])) for j in range(start, stop, step)]\n",
    "        \n",
    "        return (self.patches[i], self.__get_matching_patches(self.patches[i]))\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dir(cls, directory, file_ext, patch_width, map_transformer = None):\n",
    "        patches = []\n",
    "        \n",
    "        if file_ext == \"tif\":\n",
    "            for file in os.listdir(directory):\n",
    "                if file.endswith(\"tif\"):\n",
    "                    file_name = f\"{directory}/{file}\"\n",
    "                    logging.info(f\"Fetching patches from {file_name}.\")\n",
    "                    patches.extend(MapPatch.get_map_patch_list(file_name = file_name, \n",
    "                                                               patch_width = patch_width, \n",
    "                                                               map_transformer = map_transformer,\n",
    "                                                               verbose = True))\n",
    "        elif file_ext == \"pk\":\n",
    "            for file in os.listdir(directory):\n",
    "                if file.endswith(\"pk\"):\n",
    "                    file_name = f\"{directory}/{file}\"\n",
    "                    logging.info(f\"Fetching patches from {file_name}.\")\n",
    "                    with open(file_name, \"rb\") as f:\n",
    "                        patches.extend(pk.load(file_name))\n",
    "        else:\n",
    "            print(f\"{file_ext} is an invalid file format. Require tif or pk.\")\n",
    "            \n",
    "        return cls(patches)\n",
    "    \n",
    "    def __get_patch_dict(self, patches):\n",
    "        patch_dict = {}\n",
    "        \n",
    "        for patch in patches:\n",
    "            if patch.patch_index not in patch_dict.keys():\n",
    "                patch_dict[patch.patch_index] = []\n",
    "                \n",
    "            patch_dict[patch.patch_index].append(patch)\n",
    "        \n",
    "        return patch_dict\n",
    "    \n",
    "    def __get_matching_patches(self, patch):\n",
    "        return [match_patch for match_patch in self.patch_dict[patch.patch_index] if match_patch != patch]\n",
    "        \n",
    "    \n",
    "    def to_pickle(self, file_name = None):\n",
    "        with open(f\"{file_name}.pk\", \"wb\") as f:\n",
    "            pk.dump(self.patches, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78b6e01d-fbde-4154-9f68-fe4054142508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations to apply to the map\n",
    "\n",
    "def max_pooler(img, kernel_size):\n",
    "    return block_reduce(img, block_size = (kernel_size, kernel_size,1), func = np.max)\n",
    "\n",
    "def min_pooler(img, kernel_size):\n",
    "    return block_reduce(img, block_size = (kernel_size, kernel_size,1), func = np.min)\n",
    "\n",
    "def med_reduce(x, axis):\n",
    "    return np.median(x,axis).astype(np.int32)\n",
    "\n",
    "def med_pooler(img, kernel_size):\n",
    "    return block_reduce(img, block_size = (kernel_size, kernel_size,1), func = med_reduce)\n",
    "\n",
    "def mean_reduce(x, axis):\n",
    "    return np.mean(x,axis).astype(np.int32)\n",
    "\n",
    "def mean_pooler(img, kernel_size):\n",
    "    return block_reduce(img, block_size = (kernel_size, kernel_size,1), func = mean_reduce)\n",
    "\n",
    "def torch_downsample(img, kernel_size, interpolation = InterpolationMode.BILINEAR):\n",
    "    size = img.shape\n",
    "    \n",
    "    new_size = (size[0]//kernel_size, size[1]//kernel_size)\n",
    "    \n",
    "    tensor_img = np.moveaxis(img, -1, 0)\n",
    "    tensor_img = torch.Tensor(tensor_img)\n",
    "    \n",
    "    resized_map = T.Resize(new_size, interpolation=interpolation)(tensor_img)\n",
    "    \n",
    "    resized_map = resized_map.numpy()\n",
    "    resized_map = np.moveaxis(resized_map, 0, -1)\n",
    "    \n",
    "    return resized_map.astype(int)\n",
    "\n",
    "def bilinear_interpolator(img, kernel_size):\n",
    "    return torch_downsample(img, kernel_size, interpolation = InterpolationMode.BILINEAR)\n",
    "\n",
    "def bicubic_interpolator(img, kernel_size):\n",
    "    return torch_downsample(img, kernel_size, interpolation = InterpolationMode.BICUBIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f39a2edc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fetching patches from data/map_3.tif.\n",
      "INFO:root:Applying transformation bilinear_interpolator_4x4 to data/map_3.tif\n",
      "INFO:root:2,301 patches from data/map_3.tif generated with shape (39, 59, 1, 64, 64, 3)\n",
      "INFO:root:Fetching patches from data/map_2.tif.\n",
      "INFO:root:Applying transformation bilinear_interpolator_4x4 to data/map_2.tif\n",
      "INFO:root:2,301 patches from data/map_2.tif generated with shape (39, 59, 1, 64, 64, 3)\n",
      "INFO:root:Fetching patches from data/map_1.tif.\n",
      "INFO:root:Applying transformation bilinear_interpolator_4x4 to data/map_1.tif\n",
      "INFO:root:2,301 patches from data/map_1.tif generated with shape (39, 59, 1, 64, 64, 3)\n",
      "INFO:root:Fetching patches from data/map_4.tif.\n",
      "INFO:root:Applying transformation bilinear_interpolator_4x4 to data/map_4.tif\n",
      "INFO:root:2,301 patches from data/map_4.tif generated with shape (39, 59, 1, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "def bilinear_interpolator_4x4(img):\n",
    "    return bilinear_interpolator(img, 4)\n",
    "\n",
    "patch_dataset = CLPatchDataset.from_dir(\"data\", \n",
    "                                       file_ext = \"tif\", \n",
    "                                       patch_width = 64, \n",
    "                                       map_transformer= bilinear_interpolator_4x4)\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5cfe1f59-03f2-4d20-8a98-fb8269dd3e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapCLNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MapCLNN, self).__init__()\n",
    "        \n",
    "        # model hyperparameters\n",
    "        self.MAX_PIXEL_VALUE = 255\n",
    "        self.RESNET_DIM = 224\n",
    "        self.RESNET_OUTPUT_DIM = 1000\n",
    "        self.HIDDEN_DIM = 500\n",
    "        self.OUTPUT_DIM = 100\n",
    "        \n",
    "        # resnet for encoding input images\n",
    "        self.resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "        # network layers\n",
    "        self.lin_hidden = nn.Linear(in_features=self.RESNET_OUTPUT_DIM, out_features=self.HIDDEN_DIM, bias = False)\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features = self.HIDDEN_DIM)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin_output = n.Linear(in_features = self.HIDDEN_DIM, out_features = self.OUTPUT_DIM, bias = False)\n",
    "        \n",
    "        # define optimiser\n",
    "        self.optimiser = None\n",
    "    \n",
    "    def img_to_resnet(self, img, dim = None):\n",
    "        \"\"\"\n",
    "        Convert image into the desired format for ResNet.\n",
    "        The image must have width and height of at least self.RESNET_DIM, with RGB values between 0 and 1.\n",
    "        Moreover, it must be normalised, by using a mean of [0.485, 0.456, 0.406] and a standard deviation of [0.229, 0.224, 0.225]\n",
    "        --------------------------------------------------------------------------------------------------------------------------------\n",
    "        :param img: a numpy nd.array, with 3 colour channels (this must be stored in the last dimensions), which has to be fed to ResNet\n",
    "        :param dim: the desired dimension of the image (if we want to resize img before feeding it to ResNet).\n",
    "                    This should be at least self.RESTNET_DIM.\n",
    "        --------------------------------------------------------------------------------------------------------------------------------\n",
    "        :return a Tensor, with the first dimension corresponding to the RGB channels, and normalised to be used by ResNet.\n",
    "        \"\"\"\n",
    "        \n",
    "        # put the colour channel in front\n",
    "        if len(img.shape) == 3:\n",
    "            norm_img = np.moveaxis(img, -1, 0)\n",
    "        else:\n",
    "            norm_img = np.moveaxis(img, -1, 1)\n",
    "            \n",
    "        # normalise into range [0,1]\n",
    "        norm_img = torch.from_numpy(norm_img)/255\n",
    "        \n",
    "        # resize\n",
    "        if dim is not None:\n",
    "            assert dim >= self.RESNET_DIM, f\"Provided dimension {dim} is less than the required for RESNET ({self.RESNET_DIM})\"\n",
    "            norm_img = T.Resize(dim)(norm_img)  \n",
    "        else:\n",
    "            norm_img = T.Resize(self.RESNET_DIM)(norm_img)\n",
    "        \n",
    "        # normalise mean and variance\n",
    "        mean = torch.Tensor([0.485, 0.456, 0.406])\n",
    "        std = torch.Tensor([0.229, 0.224, 0.225])\n",
    "        \n",
    "        return T.Normalize(mean = mean, std = std)(norm_img)\n",
    "    \n",
    "    def contrastive_loss(self, z_batch, tau):\n",
    "        \"\"\"\n",
    "        Computes the contrastive loss (NT-XENT) for a mini-batch of augmented samples.\n",
    "        --------------------------------------------------------------------------------------------------------\n",
    "        z_batch: a (N,K) Tensor, with rows as embedding vectors. \n",
    "                 We expect that z_batch[2k] and z_batch[2k+1], 0 <= k < N, correspond to a positive sample pair\n",
    "        tau: temperature parameter for NT-XENT loss\n",
    "        --------------------------------------------------------------------------------------------------------\n",
    "        return: a float, corresponding to the total loss for the mini-batch z_batch\n",
    "        \"\"\"\n",
    "        N = len(z_batch)\n",
    "\n",
    "        # normalise to have unit length rows\n",
    "        norm_z_batch = F.normalize(z_batch)\n",
    "\n",
    "        # compute similarity & apply factor of tau\n",
    "        sim_batch = (norm_z_batch @ norm_z_batch.T)/tau\n",
    "\n",
    "        # fill the diagonal with -1000, to make sure it is never considered in the cross entropy computations\n",
    "        sim_batch.fill_diagonal_(-1000)\n",
    "\n",
    "        # generate labels\n",
    "        # z_batch[2k] should be similar to z_batch[2k+1] (since these will be the positive pair)\n",
    "        # hence, labels should have the form [1,0,3,2,...,N,N-1]\n",
    "        labels = torch.Tensor([k+1 if k%2 == 0 else k-1 for k in range(0,N)]).long()\n",
    "\n",
    "        # return the NT-XENT loss\n",
    "        return 1/N * F.cross_entropy(sim_batch, labels, reduction = \"sum\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        A forward pass through the network\n",
    "        \"\"\"\n",
    "        res_x = self.img_to_resnet(x)\n",
    "        \n",
    "        model = nn.Sequential(\n",
    "                self.resnet,\n",
    "                self.lin_hidden,\n",
    "                self.batch_norm,\n",
    "                self.relu,\n",
    "                self.lin_output\n",
    "                )\n",
    "        \n",
    "        return model(res_x)\n",
    "        \n",
    "    \n",
    "    def compile_optimiser(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Sets the optimiser parameters.\n",
    "        \"\"\"\n",
    "        self.optimiser = optim.Adam(self.parameters(), **kwargs)\n",
    "    \n",
    "    def train(self, dataloader, tau, epochs):\n",
    "        \"\"\"\n",
    "        Trains the network.\n",
    "        \"\"\"\n",
    "        for epoch in epochs:\n",
    "            for x_1,x_2 in dataloader:\n",
    "                self.optimiser.zero_grad()\n",
    "                \n",
    "                z_1 = self(x_1)\n",
    "                z_2 = self(x_2)\n",
    "                \n",
    "                z_batch = torch.stack((z_1,z_2), dim = 1).view(-1, self.OUTPUT_DIM)\n",
    "                loss = self.contrastive_loss(z_batch, tau = tau)\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimiser.step()\n",
    "                \n",
    "            if epoch % (epochs // 10) == 0:\n",
    "                print(f\"Epoch {epoch + 1} ---- NT-XENT = {loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "henv",
   "language": "python",
   "name": "henv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
