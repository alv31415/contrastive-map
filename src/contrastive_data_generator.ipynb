{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e65753a2-eb15-4816-af0b-a1749ebbdefc",
   "metadata": {},
   "source": [
    "1) Data is tif files stored in directory src/data/originals\n",
    "2) Maps corresponding to the same region are stored in the same folder\n",
    "3) Convert each of the maps to patches\n",
    "4) Apply the alignment (use the oldest map as the reference map - the one with the smalles)\n",
    "5) Format to feed to network (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58795690-75d4-42be-93ad-b7a5c007b700",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8996c32-7aa0-485c-9b57-bdb536836a79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "import os\n",
    "import logging\n",
    "import pickle as pk\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from patchify import patchify,unpatchify\n",
    "import cv2 as cv\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)-4s %(message)s',\n",
    "                    level=logging.INFO,\n",
    "                    datefmt='%d-%m-%Y %H:%M:%S')\n",
    "\n",
    "logging.getLogger(\"PIL\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"matplotlib\").setLevel(logging.WARNING)\n",
    "\n",
    "PIL.Image.MAX_IMAGE_PIXELS = 933120000\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from skimage.measure import block_reduce\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "from torchvision.models.resnet import resnet50, ResNet50_Weights\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea7d9a9-40a7-486d-b8fc-4b6b410b173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2383d7-89e0-41a1-9c85-00fb33537ad5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Class: MapPatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75db2b76-cd1e-48ee-b5a8-3c91b9224ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapPatch():\n",
    "    def __init__(self, patch, patch_index, origin_map):\n",
    "        self.patch = patch\n",
    "        self.patch_index = patch_index\n",
    "        self.origin_map = origin_map\n",
    "        self.patch_shift = None\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_map_patches(file_name, patch_width, verbose = True):\n",
    "        tif_map = PIL.Image.open(file_name)\n",
    "        tif_map_np = np.array(tif_map)\n",
    "        \n",
    "        tif_map_patches = patchify(image = tif_map_np, \n",
    "                                   patch_size = (patch_width, patch_width, 3),\n",
    "                                   step = patch_width)\n",
    "\n",
    "        if verbose:\n",
    "            logging.info(f\"{np.prod(tif_map_patches.shape[:2]):,} patches from {file_name} generated with shape {tif_map_patches.shape}\")\n",
    "\n",
    "        return tif_map_np, tif_map_patches\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_map_patch_list(file_name, patch_width, verbose = True):\n",
    "        _, tif_map_patches = MapPatch.get_map_patches(file_name, \n",
    "                                                      patch_width, \n",
    "                                                      verbose = verbose)\n",
    "        \n",
    "        patches = [MapPatch(tif_map_patches[i,j,0], patch_index = (i,j), origin_map = file_name)\n",
    "                  for i in range(tif_map_patches.shape[0])\n",
    "                  for j in range(tif_map_patches.shape[1])]\n",
    "                \n",
    "        return patches\n",
    "    \n",
    "    def get_map_px(self):\n",
    "        \"\"\"\n",
    "        Gets location of the top left pixel of the patch in the original image.\n",
    "        \"\"\"\n",
    "        patch_size = self.patch.shape[0]\n",
    "        row = self.patch_index[0]\n",
    "        col = self.patch_index[1]\n",
    "        \n",
    "        return (row * patch_size, col * patch_size)\n",
    "    \n",
    "    def show(self, verbose = True):\n",
    "        \"\"\"\n",
    "        Shows the patch.\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(self.patch)\n",
    "        \n",
    "        if verbose:\n",
    "            ax.set_title(f\"Patch at {self.patch_index} from {self.origin_map}.\")\n",
    "            \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86e396e-c16f-48ae-8787-89e2c251bd3c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Class: CLPatchDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8274fc7f-0860-4dba-9b5f-7331eca27ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLPatchDataset(Dataset):\n",
    "    def __init__(self, X_1, X_2):\n",
    "        self.X_1 = X_1\n",
    "        self.X_2 = X_2\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X_1)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        if isinstance(i, slice):\n",
    "            start = i.start if i.start else 0\n",
    "            stop = i.stop if i.stop else len(self.patches)\n",
    "            step = i.step if i.step else 1\n",
    "            \n",
    "            return [(self.X_1[j], self.X_2[j]) for j in range(start, stop, step)]\n",
    "        \n",
    "        return (self.X_1[i].patch, self.X_2[i].patch)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dir(cls, map_directory, patch_width, verbose = False):\n",
    "        X_1 = []\n",
    "        X_2 = []\n",
    "\n",
    "        for folder in os.listdir(map_directory):\n",
    "            if folder.isdigit():\n",
    "                if verbose:\n",
    "                    logging.info(f\"Fetching patches from folder: {folder}\")\n",
    "                    \n",
    "                directory = os.path.join(map_directory, folder)\n",
    "                patch_list = CLPatchDataset.get_patch_list_from_dir(directory, patch_width = patch_width, verbose = False)\n",
    "                x_1, x_2 = CLPatchDataset.get_matching_patch_list(patch_list)\n",
    "                X_1.extend(x_1)\n",
    "                X_2.extend(x_2)\n",
    "            \n",
    "        return cls(X_1, X_2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def index_sampler(indices, n_samples = 4):\n",
    "        return [indices[i] for i in np.random.choice(len(indices), n_samples, replace=False)]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_patch_list_from_dir(directory, patch_width, verbose = False):\n",
    "        patches = []\n",
    "\n",
    "        for file in os.listdir(directory):\n",
    "            if file.endswith(\"tif\"):\n",
    "                file_name = f\"{directory}/{file}\"\n",
    "                patches.append(MapPatch.get_map_patch_list(file_name = file_name, \n",
    "                                                           patch_width = patch_width, \n",
    "                                                           verbose = verbose))\n",
    "\n",
    "        return patches\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_empty_patch(patch):\n",
    "        gray = cv.cvtColor(np.array(patch), cv.COLOR_RGB2GRAY)\n",
    "        gray = cv.GaussianBlur(gray, (3,3), 0)\n",
    "        edges = cv.Canny(gray, 50, 150)\n",
    "        black_pixels = np.where(edges == 255)\n",
    "\n",
    "        return len(black_pixels[0]) <= 20\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_matching_patch_list(patch_list):\n",
    "        n_samples = len(patch_list)\n",
    "        indices = [(i,j) for i in range(n_samples) for j in range(i) if i != j]\n",
    "\n",
    "        x_1 = []\n",
    "        x_2 = []\n",
    "\n",
    "        for i in range(len(patch_list[0])):\n",
    "            #sample_indices = CLPatchDataset.index_sampler(indices, n_samples)\n",
    "            sample_indices = [(i,i+1) for i in range(n_samples-1)]\n",
    "            for index in sample_indices:\n",
    "                try:\n",
    "                    patch_1 = patch_list[index[0]][i]\n",
    "                    patch_2 = patch_list[index[1]][i]\n",
    "                    if not CLPatchDataset.is_empty_patch(patch_1.patch) and not CLPatchDataset.is_empty_patch(patch_2.patch):\n",
    "                        x_1.append(patch_1)\n",
    "                        x_2.append(patch_2)\n",
    "                except IndexError:\n",
    "                    print(f\"Faulty generated index: {index}\")\n",
    "                    print(f\"Index i: {i}\")\n",
    "                    print(f\"len(patch_list): {len(patch_list)}\")\n",
    "                    print(f\"len(patch_list[index[0]]): {len(patch_list[index[0]])}\")\n",
    "                    print(f\"len(patch_list[index[1]]): {len(patch_list[index[1]])}\")\n",
    "                    raise ValueError()\n",
    "\n",
    "        return x_1, x_2\n",
    "    \n",
    "    def save(self, file_name):\n",
    "        with open(f\"{file_name}.pk\", \"wb\") as f:\n",
    "            pk.dump(self, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7010d78-2fe8-405d-bd77-ce5449f5ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_patch_dataset = CLPatchDataset.from_dir(\"../src/data/originals/\", 64, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace0cb9b-acbd-4e91-ab4d-efd5c0c2c860",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_patch_dataset.save(\"../src/data/originals/patch_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8bffca-7208-4ae1-b24c-3331834a2bae",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Utility Classes for NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d157dc9b-953a-4c74-a6e2-4f25cacfd65b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Class: MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98e935b-1dfd-46fe-a89a-6c81cbf22e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Class encoding a simple multi-layer perceptron, used for encoding and prediction.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, activation = nn.ReLU(), use_bias = True, use_batch_norm = True):\n",
    "        \"\"\"\n",
    "        input_dim: an int, indicating the input dimension for the MLP.\n",
    "        hidden_dim: an int, indicating the hidden dimension for the MLP.\n",
    "        output_dim: an int, indicating the output dimension for the MLP.\n",
    "        activation: a Pytorch activation (from the nn module), providing the non-linearity for the MLP.\n",
    "        use_bias: a boolean. If true, the linear layers will use a bias weight.\n",
    "        use_batch_norm: a boolean. If true, applies batch normalisation after the hidden layer.\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # network layers for the projection head\n",
    "        self.lin_hidden = nn.Linear(in_features = input_dim, \n",
    "                                    out_features = hidden_dim, \n",
    "                                    bias = use_bias)\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features = hidden_dim) if use_batch_norm else nn.Identity()\n",
    "        self.activation = activation\n",
    "        self.lin_output = nn.Linear(in_features = hidden_dim, \n",
    "                                    out_features = output_dim, \n",
    "                                    bias = use_bias)\n",
    "        \n",
    "        # define the model\n",
    "        self.mlp = nn.Sequential(\n",
    "                self.lin_hidden,\n",
    "                self.batch_norm,\n",
    "                self.activation,\n",
    "                self.lin_output\n",
    "                )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        A forward pass through the MLP.\n",
    "        \"\"\"\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a5b826-fc02-44d3-ac03-7673ce58b13e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Class: Hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27fed03-c9e5-44fb-b6bb-4281c47d27d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the Hook class by Kennethborup:\n",
    "# https://github.com/Kennethborup/BYOL\n",
    "class Hook():\n",
    "    \"\"\"\n",
    "    A simple hook class that returns the output of a layer of a model during forward pass.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.output = None\n",
    "        \n",
    "    def set_hook(self, module):\n",
    "        \"\"\"\n",
    "        Attaches hook to model.\n",
    "        \"\"\"\n",
    "        self.hook = module.register_forward_hook(self.hook_fn)\n",
    "\n",
    "    def hook_fn(self, module, _, output):\n",
    "        \"\"\"\n",
    "        Saves the wanted information.\n",
    "        \"\"\"\n",
    "        self.output = output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3065c732-85ea-4e8d-abb0-d69ef4439709",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Class: EncoderProjectorNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ace660-a41d-44cb-a6f9-59abca88a631",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderProjectorNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Class for a network which encodes an input and projects it into a latent space.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, projector, encoder_layer_idx = -1):\n",
    "        \"\"\"\n",
    "        encoder: a nn.Module, containing an encoder network.\n",
    "        projector: a nn.Module, containing a projector network.\n",
    "        encoder_layer_idx: an int, corresponding to the index of the layer of the encoder \n",
    "                           which is actually used for encoding.\n",
    "                           For instance, in BYOL, if using a ResNet as an encoder, \n",
    "                           the paper uses the output of the last average pooliing layer, \n",
    "                           which is the penultimate layer of the ResNet \n",
    "                           (corresponding to encoder_layer_idx = -2).\n",
    "        \"\"\"\n",
    "        super(EncoderProjectorNN, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.projector = projector\n",
    "        self.encoder_layer_idx = encoder_layer_idx\n",
    "        \n",
    "        # if the encoding layer isn't the last one, add a hook to save the output of the layer\n",
    "        if self.encoder_layer_idx != -1:\n",
    "            self.hook = Hook()\n",
    "            layers = [*self.encoder.children()]\n",
    "            encoder_layer = layers[self.encoder_layer_idx]\n",
    "            self.hook.set_hook(encoder_layer)\n",
    "            \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encodes an input x, according to the encoder network and self.encoder_layer_idx.\n",
    "        \"\"\"\n",
    "        \n",
    "        encoded = self.encoder(x)\n",
    "        \n",
    "        if self.encoder_layer_idx != -1:\n",
    "            # need to fix the output to remove the additional dimensions\n",
    "            # for instance, images sent as 4-dimensional input (N,C,W,H) will remain 4 dimensional \n",
    "            # after passing through the encoder, and rely on final layers to reduce this to\n",
    "            # 2 dimensions (N,X). \n",
    "            # Thus, if self.encoder_layer_idx != -1, we are missing this dimensionality reduction step, \n",
    "            # so we need to compensate for it.\n",
    "            encoded = self.hook.output.reshape(encoded.shape[0],-1)\n",
    "            \n",
    "        return encoded\n",
    "    \n",
    "    def project(self, x):\n",
    "        \"\"\"\n",
    "        Projects an input x into latent space, according to the projector network.\n",
    "        \"\"\"\n",
    "        return self.projector(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        A forward pass through the model, involving first encoding and then projecting.\n",
    "        \"\"\"\n",
    "        encode_x = self.encode(x)\n",
    "        return self.project(encode_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d632885-8400-4325-9059-0b5361e84a1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Class: SIMCLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041036f5-00b5-4911-955a-7c930fd5bc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapSIMCLR(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MapSIMCLR, self).__init__()\n",
    "        \n",
    "        # model hyperparameters\n",
    "        self.MAX_PIXEL_VALUE = 255\n",
    "        self.RESNET_DIM = 224\n",
    "        self.RESNET_OUTPUT_DIM = 1000\n",
    "        self.HIDDEN_DIM = 1500\n",
    "        self.OUTPUT_DIM = 100\n",
    "        \n",
    "        # resnet for encoding input images\n",
    "        self.resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "        # network layers for the projection head\n",
    "        self.lin_hidden = nn.Linear(in_features=self.RESNET_OUTPUT_DIM, out_features=self.HIDDEN_DIM, bias = False)\n",
    "        self.batch_norm = nn.BatchNorm1d(num_features = self.HIDDEN_DIM)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin_output = nn.Linear(in_features = self.HIDDEN_DIM, out_features = self.OUTPUT_DIM, bias = False)\n",
    "        \n",
    "        # define the model\n",
    "        self.model = nn.Sequential(\n",
    "                self.resnet,\n",
    "                self.lin_hidden,\n",
    "                self.batch_norm,\n",
    "                self.relu,\n",
    "                self.lin_output\n",
    "                )\n",
    "        \n",
    "        # define optimiser\n",
    "        self.optimiser = None\n",
    "\n",
    "        # get the device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # create checkpoint\n",
    "        self.checkpoint = {\"epoch\" : 0,\n",
    "                           \"batch\" : 0,\n",
    "                           \"model_state_dict\" : self.state_dict(),\n",
    "                           \"optimiser_state_dict\": None,\n",
    "                           \"loss\" : 0,\n",
    "                           \"avg_loss_20\" : 0,\n",
    "                           \"run_start\" : datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"),\n",
    "                           \"run_end\" : None}\n",
    "    \n",
    "    def img_to_resnet(self, img, dim = None):\n",
    "        \"\"\"\n",
    "        Convert image into the desired format for ResNet.\n",
    "        The image must have width and height of at least self.RESNET_DIM, with RGB values between 0 and 1.\n",
    "        Moreover, it must be normalised, by using a mean of [0.485, 0.456, 0.406] and a standard deviation of [0.229, 0.224, 0.225]\n",
    "        --------------------------------------------------------------------------------------------------------------------------------\n",
    "        :param img: a numpy nd.array, with 3 colour channels (this must be stored in the last dimensions), which has to be fed to ResNet\n",
    "        :param dim: the desired dimension of the image (if we want to resize img before feeding it to ResNet).\n",
    "                    This should be at least self.RESTNET_DIM.\n",
    "        --------------------------------------------------------------------------------------------------------------------------------\n",
    "        :return a Tensor, with the first dimension corresponding to the RGB channels, and normalised to be used by ResNet.\n",
    "        \"\"\"\n",
    "        \n",
    "        # put the colour channel in front and normalise into range [0,1]\n",
    "        if len(img.shape) == 3:\n",
    "            norm_img = torch.moveaxis(img, -1, 0)/self.MAX_PIXEL_VALUE\n",
    "        else:\n",
    "            norm_img = torch.moveaxis(img, -1, 1)/self.MAX_PIXEL_VALUE\n",
    "        \n",
    "        # resize\n",
    "        if dim is not None:\n",
    "            assert dim >= self.RESNET_DIM, f\"Provided dimension {dim} is less than the required for RESNET ({self.RESNET_DIM})\"\n",
    "            norm_img = T.Resize(dim)(norm_img)  \n",
    "        else:\n",
    "            norm_img = T.Resize(self.RESNET_DIM)(norm_img)\n",
    "        \n",
    "        # normalise mean and variance\n",
    "        mean = torch.Tensor([0.485, 0.456, 0.406])\n",
    "        std = torch.Tensor([0.229, 0.224, 0.225])\n",
    "        \n",
    "        return T.Normalize(mean = mean, std = std)(norm_img)\n",
    "    \n",
    "    def contrastive_loss(self, z_batch, tau):\n",
    "        \"\"\"\n",
    "        Computes the contrastive loss (NT-XENT) for a mini-batch of augmented samples.\n",
    "        --------------------------------------------------------------------------------------------------------\n",
    "        z_batch: a (N,K) Tensor, with rows as embedding vectors. \n",
    "                 We expect that z_batch[2k] and z_batch[2k+1], 0 <= k < N, correspond to a positive sample pair\n",
    "        tau: temperature parameter for NT-XENT loss\n",
    "        --------------------------------------------------------------------------------------------------------\n",
    "        return: a float, corresponding to the total loss for the mini-batch z_batch\n",
    "        \"\"\"\n",
    "        N = len(z_batch)\n",
    "\n",
    "        # normalise to have unit length rows\n",
    "        norm_z_batch = F.normalize(z_batch)\n",
    "\n",
    "        # compute similarity & apply factor of tau\n",
    "        sim_batch = (norm_z_batch @ norm_z_batch.T)/tau\n",
    "\n",
    "        # fill the diagonal with -1000, to make sure it is never considered in the cross entropy computations\n",
    "        sim_batch.fill_diagonal_(-1000)\n",
    "\n",
    "        # generate labels\n",
    "        # z_batch[2k] should be similar to z_batch[2k+1] (since these will be the positive pair)\n",
    "        # hence, labels should have the form [1,0,3,2,...,N,N-1]\n",
    "        labels = torch.Tensor([k+1 if k%2 == 0 else k-1 for k in range(0,N)]).long().to(self.device)\n",
    "\n",
    "        # return the NT-XENT loss\n",
    "        return 1/N * F.cross_entropy(sim_batch, labels, reduction = \"sum\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        A forward pass through the network\n",
    "        \"\"\"\n",
    "        res_x = self.img_to_resnet(x)\n",
    "        \n",
    "        return self.model(res_x)\n",
    "        \n",
    "    \n",
    "    def compile_optimiser(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Sets the optimiser parameters.\n",
    "        \"\"\"\n",
    "        self.optimiser = optim.Adam(self.parameters(), **kwargs)\n",
    "        \n",
    "    def update_checkpoint(self, **checkpoint_data):\n",
    "        \"\"\"\n",
    "        Updates the checkpoint dictionary.\n",
    "        \"\"\"\n",
    "        for k,v in checkpoint_data:\n",
    "            if k in self.checkpoint:\n",
    "                self.checkpoint[k] = v\n",
    "    \n",
    "    def train(self, dataloader, tau, epochs):\n",
    "        \"\"\"\n",
    "        Trains the network.\n",
    "        \"\"\"\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            batch_losses = []\n",
    "            for batch, (x_1,x_2) in enumerate(dataloader):\n",
    "                # x_1 and x_2 are tensors containing patches, \n",
    "                # such that x_1[i] and x_2[i] are patches for the same area\n",
    "                \n",
    "                self.optimiser.zero_grad()\n",
    "\n",
    "                x_1, x_2 = x_1.to(self.device), x_2.to(self.device)\n",
    "                \n",
    "                z_1 = self(x_1)\n",
    "                z_2 = self(x_2)\n",
    "                \n",
    "                z_batch = torch.stack((z_1,z_2), dim = 1).view(-1, self.OUTPUT_DIM)\n",
    "                loss = self.contrastive_loss(z_batch, tau = tau)\n",
    "\n",
    "                batch_losses.append(loss.cpu())\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimiser.step()\n",
    "\n",
    "                if batch % (len(dataloader) // 100 + 1) == 0:\n",
    "                    with torch.no_grad():\n",
    "                        avg_loss = np.mean(batch_losses[-20:])\n",
    "                        print(f\"Epoch {epoch + 1}: [{batch + 1}/{len(dataloader)}] ---- NT-XENT = {avg_loss}\")\n",
    "                        \n",
    "                        self.update_checkpoint(epoch = epoch,\n",
    "                                   batch = batch,\n",
    "                                   model_state_dict = self.state_dict(),\n",
    "                                   optimiser_state_dict = self.optimiser.state_dict,\n",
    "                                   loss = loss.cpu(),\n",
    "                                   avg_loss_20 = avg_loss,\n",
    "                                   run_end = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            self.update_checkpoint(epoch = epochs,\n",
    "                                   batch = len(dataloader),\n",
    "                                   model_state_dict = self.state_dict(),\n",
    "                                   optimiser_state_dict = self.optimiser.state_dict,\n",
    "                                   loss = loss.cpu(),\n",
    "                                   avg_loss_20 = np.mean(batch_losses[-20:]),\n",
    "                                   run_end = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "        \n",
    "        return self.checkpoint\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001a1643-971e-4c38-8915-f9cc36fc1455",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapSIMCLR(nn.Module):\n",
    "    def __init__(self, encoder, encoder_layer_idx, projector_parameters, tau):\n",
    "        \"\"\"\n",
    "        tau: temperature parameter for NT-XENT loss\n",
    "        \"\"\"\n",
    "        super(MapSIMCLR, self).__init__()\n",
    "        \n",
    "        # model constants\n",
    "        self.MAX_PIXEL_VALUE = 255\n",
    "        self.RESNET_DIM = 224\n",
    "        \n",
    "        # define the model\n",
    "        self.model = EncoderProjectorNN(encoder = encoder,\n",
    "                                        projector = MLP(**projector_parameters),\n",
    "                                        encoder_layer_idx = encoder_layer_idx)\n",
    "        \n",
    "        self.tau = tau\n",
    "        \n",
    "        # define optimiser\n",
    "        self.optimiser = None\n",
    "\n",
    "        # get the device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # create checkpoint\n",
    "        self.checkpoint = {\"epoch\" : 0,\n",
    "                           \"batch\" : 0,\n",
    "                           \"model_state_dict\" : self.state_dict(),\n",
    "                           \"optimiser_state_dict\": None,\n",
    "                           \"loss\" : 0,\n",
    "                           \"avg_loss_20\" : 0,\n",
    "                           \"run_start\" : datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"),\n",
    "                           \"run_end\" : None}\n",
    "    \n",
    "    def img_to_resnet(self, img, dim = None):\n",
    "        \"\"\"\n",
    "        Convert image into the desired format for ResNet.\n",
    "        The image must have width and height of at least self.RESNET_DIM, with RGB values between 0 and 1.\n",
    "        Moreover, it must be normalised, by using a mean of [0.485, 0.456, 0.406] and a standard deviation of [0.229, 0.224, 0.225]\n",
    "        --------------------------------------------------------------------------------------------------------------------------------\n",
    "        :param img: a numpy nd.array, with 3 colour channels (this must be stored in the last dimensions), which has to be fed to ResNet\n",
    "        :param dim: the desired dimension of the image (if we want to resize img before feeding it to ResNet).\n",
    "                    This should be at least self.RESTNET_DIM.\n",
    "        --------------------------------------------------------------------------------------------------------------------------------\n",
    "        :return a Tensor, with the first dimension corresponding to the RGB channels, and normalised to be used by ResNet.\n",
    "        \"\"\"\n",
    "        \n",
    "        # put the colour channel in front and normalise into range [0,1]\n",
    "        if len(img.shape) == 3:\n",
    "            norm_img = torch.moveaxis(img, -1, 0)/self.MAX_PIXEL_VALUE\n",
    "        else:\n",
    "            norm_img = torch.moveaxis(img, -1, 1)/self.MAX_PIXEL_VALUE\n",
    "        \n",
    "        # resize\n",
    "        if dim is not None:\n",
    "            assert dim >= self.RESNET_DIM, f\"Provided dimension {dim} is less than the required for RESNET ({self.RESNET_DIM})\"\n",
    "            norm_img = T.Resize(dim)(norm_img)  \n",
    "        else:\n",
    "            norm_img = T.Resize(self.RESNET_DIM)(norm_img)\n",
    "        \n",
    "        # normalise mean and variance\n",
    "        mean = torch.Tensor([0.485, 0.456, 0.406])\n",
    "        std = torch.Tensor([0.229, 0.224, 0.225])\n",
    "        \n",
    "        return T.Normalize(mean = mean, std = std)(norm_img)\n",
    "    \n",
    "    def contrastive_loss(self, z_batch):\n",
    "        \"\"\"\n",
    "        Computes the contrastive loss (NT-XENT) for a mini-batch of augmented samples.\n",
    "        --------------------------------------------------------------------------------------------------------\n",
    "        z_batch: a (N,K) Tensor, with rows as embedding vectors. \n",
    "                 We expect that z_batch[2k] and z_batch[2k+1], 0 <= k < N, correspond to a positive sample pair\n",
    "        --------------------------------------------------------------------------------------------------------\n",
    "        return: a float, corresponding to the total loss for the mini-batch z_batch\n",
    "        \"\"\"\n",
    "        N = len(z_batch)\n",
    "\n",
    "        # normalise to have unit length rows\n",
    "        norm_z_batch = F.normalize(z_batch)\n",
    "\n",
    "        # compute similarity & apply factor of tau\n",
    "        sim_batch = (norm_z_batch @ norm_z_batch.T)/self.tau\n",
    "\n",
    "        # fill the diagonal with -1000, to make sure it is never considered in the cross entropy computations\n",
    "        sim_batch.fill_diagonal_(-1000)\n",
    "\n",
    "        # generate labels\n",
    "        # z_batch[2k] should be similar to z_batch[2k+1] (since these will be the positive pair)\n",
    "        # hence, labels should have the form [1,0,3,2,...,N,N-1]\n",
    "        labels = torch.Tensor([k+1 if k%2 == 0 else k-1 for k in range(0,N)]).long().to(self.device)\n",
    "\n",
    "        # return the NT-XENT loss\n",
    "        return 1/N * F.cross_entropy(sim_batch, labels, reduction = \"sum\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        A forward pass through the network\n",
    "        \"\"\"\n",
    "        \n",
    "        if x.shape[:2] != (self.RESNET_DIM, self.RESNET_DIM):\n",
    "            x = self.img_to_resnet(x)\n",
    "            \n",
    "        return self.model.encode(x)\n",
    "        \n",
    "    \n",
    "    def compile_optimiser(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Sets the optimiser parameters.\n",
    "        \"\"\"\n",
    "        self.optimiser = optim.Adam(self.parameters(), **kwargs)\n",
    "        \n",
    "    def update_checkpoint(self, **checkpoint_data):\n",
    "        \"\"\"\n",
    "        Updates the checkpoint dictionary.\n",
    "        \"\"\"\n",
    "        for k,v in checkpoint_data:\n",
    "            if k in self.checkpoint:\n",
    "                self.checkpoint[k] = v\n",
    "    \n",
    "    def train(self, dataloader, epochs, transform = None):\n",
    "        \"\"\"\n",
    "        Trains the network.\n",
    "        \"\"\"\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            batch_losses = []\n",
    "            for batch, (x_1,x_2) in enumerate(dataloader):\n",
    "                # x_1 and x_2 are tensors containing patches, \n",
    "                # such that x_1[i] and x_2[i] are patches for the same area\n",
    "                \n",
    "                self.optimiser.zero_grad()\n",
    "\n",
    "                x_1, x_2 = transform(x_1.to(self.device)), transform(x_2.to(self.device))\n",
    "                \n",
    "                z_1 = self.model(x_1)\n",
    "                z_2 = self.model(x_2)\n",
    "                \n",
    "                z_batch = torch.stack((z_1,z_2), dim = 1).view(-1, self.OUTPUT_DIM)\n",
    "                loss = self.contrastive_loss(z_batch, tau = tau)\n",
    "\n",
    "                batch_losses.append(loss.cpu())\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimiser.step()\n",
    "\n",
    "                if batch % (len(dataloader) // 100 + 1) == 0:\n",
    "                    with torch.no_grad():\n",
    "                        avg_loss = np.mean(batch_losses[-20:])\n",
    "                        print(f\"Epoch {epoch + 1}: [{batch + 1}/{len(dataloader)}] ---- NT-XENT = {avg_loss}\")\n",
    "                        \n",
    "                        self.update_checkpoint(epoch = epoch,\n",
    "                                   batch = batch,\n",
    "                                   model_state_dict = self.state_dict(),\n",
    "                                   optimiser_state_dict = self.optimiser.state_dict,\n",
    "                                   loss = loss.cpu(),\n",
    "                                   avg_loss_20 = avg_loss,\n",
    "                                   run_end = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            self.update_checkpoint(epoch = epochs,\n",
    "                                   batch = len(dataloader),\n",
    "                                   model_state_dict = self.state_dict(),\n",
    "                                   optimiser_state_dict = self.optimiser.state_dict,\n",
    "                                   loss = loss.cpu(),\n",
    "                                   avg_loss_20 = np.mean(batch_losses[-20:]),\n",
    "                                   run_end = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "        \n",
    "        return self.checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b39756-eca8-4cae-ab02-7cbce19b7766",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Class: MapBYOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef88a28-8f68-4a66-b1c8-0cf37f9b1a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapBYOL(nn.Module):\n",
    "    \"\"\"\n",
    "    The BYOL model consists on 2 networks: an online and a target network.\n",
    "    Traditionally, we have an input image x, which is then transformed into positive pairs v and v'.\n",
    "    We train the online network, which consists of:\n",
    "        - an encoder (which converts v to a vector representation y = f(v))\n",
    "        - a projector (which projects y into latent space representation z = g(y))\n",
    "        - a predictor (which predicts the latent representation for v' from v_sim = q(z))\n",
    "    The latent representation for v' is obtained by applying the target network to v'.\n",
    "    However, we don't directly train this target network; \n",
    "    instead, we use an EXPONENTIAL MOVING AVERAGE of the weights for the online network.\n",
    "    The loss for this involves applying a cosine similarity (which is L2 normalised) between \n",
    "    v_sim and the latent representation for v'. Since the model isn't symmetric, \n",
    "    we compute the same loss, but this time passing v' through the online network, \n",
    "    and v through the target network. We then add these 2 losses to obtain the BYOL loss.\n",
    "    Critically, we only compute gradients with respect to the parameters of the online network\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, encoder_layer_idx, projector_parameters, predictor_parameters, ema_tau):\n",
    "        \"\"\"\n",
    "        encoder: a nn.Module, containing an encoder network.\n",
    "        encoder_layer_idx: an int, corresponding to the index of the layer of the encoder \n",
    "                           which is actually used for encoding.\n",
    "                           For instance, in BYOL, if using a ResNet as an encoder, \n",
    "                           the paper uses the output of the last average pooliing layer, \n",
    "                           which is the penultimate layer of the ResNet \n",
    "                           (corresponding to encoder_layer_idx = -2).\n",
    "        projector_parameters: a dict, containing the parameters to initialise an MLP to act as \n",
    "                              a projector network.\n",
    "        projector_parameters: a dict, containing the parameters to initialise an MLP to act as \n",
    "                              a predictor network.\n",
    "        ema_tau: a float (between 0 and 1). The constant used to compute the exponential moving \n",
    "                 average when deriving the target network parameters.\n",
    "        \"\"\"\n",
    "        super(MapBYOL, self).__init__()\n",
    "        \n",
    "        # model constants\n",
    "        self.MAX_PIXEL_VALUE = 255\n",
    "        self.RESNET_DIM = 224\n",
    "        \n",
    "        self.ema_tau = ema_tau\n",
    "        \n",
    "        # define networks\n",
    "        self.online_network = EncoderProjectorNN(encoder = encoder,\n",
    "                                                 projector = MLP(**projector_parameters),\n",
    "                                                 encoder_layer_idx = encoder_layer_idx)\n",
    "        \n",
    "        self.target_network = deepcopy(self.online_network)\n",
    "        \n",
    "        self.online_predictor = MLP(**predictor_parameters)\n",
    "\n",
    "        # define optimiser\n",
    "        self.optimiser = None\n",
    "\n",
    "        # get the device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # create checkpoint\n",
    "        self.checkpoint = {\"epoch\" : 0,\n",
    "                           \"batch\" : 0,\n",
    "                           \"model_state_dict\" : self.state_dict(),\n",
    "                           \"optimiser_state_dict\": None,\n",
    "                           \"loss\" : 0,\n",
    "                           \"avg_loss_20\" : 0,\n",
    "                           \"run_start\" : datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"),\n",
    "                           \"run_end\" : None}\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def update_target_network(self):\n",
    "        \"\"\"\n",
    "        Updates the target network parameters by using an exponential moving average \n",
    "        of the online network parameters.\n",
    "        \"\"\"\n",
    "        for online_params, target_params in zip(self.online_network.parameters(), self.target_network.parameters()):\n",
    "            target_params.data = self.ema_tau * target_params.data + (1 - self.ema_tau) * online_params.data\n",
    "    \n",
    "    def img_to_resnet(self, img, dim = None):\n",
    "        \"\"\"\n",
    "        Convert image into the desired format for ResNet.\n",
    "        The image must have width and height of at least self.RESNET_DIM, with RGB values between 0 and 1.\n",
    "        Moreover, it must be normalised, by using a mean of [0.485, 0.456, 0.406] and a standard deviation \n",
    "        of [0.229, 0.224, 0.225]\n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        :param img: a numpy nd.array, with 3 colour channels (this must be stored in the last dimensions), \n",
    "        which has to be fed to ResNet\n",
    "        :param dim: the desired dimension of the image (if we want to resize img before feeding it to \n",
    "                    ResNet). This should be at least self.RESTNET_DIM.\n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        :return a Tensor, with the first dimension corresponding to the RGB channels, and normalised to be \n",
    "                used by ResNet.\n",
    "        \"\"\"\n",
    "        \n",
    "        # put the colour channel in front and normalise into range [0,1]\n",
    "        if len(img.shape) == 3:\n",
    "            norm_img = torch.moveaxis(img, -1, 0)/self.MAX_PIXEL_VALUE\n",
    "        else:\n",
    "            norm_img = torch.moveaxis(img, -1, 1)/self.MAX_PIXEL_VALUE\n",
    "        \n",
    "        # resize\n",
    "        if dim is not None:\n",
    "            assert dim >= self.RESNET_DIM, f\"Provided dimension {dim} is less than the required for RESNET ({self.RESNET_DIM})\"\n",
    "            norm_img = T.Resize(dim)(norm_img)  \n",
    "        else:\n",
    "            norm_img = T.Resize(self.RESNET_DIM)(norm_img)\n",
    "        \n",
    "        # normalise mean and variance\n",
    "        mean = torch.Tensor([0.485, 0.456, 0.406])\n",
    "        std = torch.Tensor([0.229, 0.224, 0.225])\n",
    "        \n",
    "        return T.Normalize(mean = mean, std = std)(norm_img)\n",
    "    \n",
    "    def byol_loss(self, x_1, x_2):\n",
    "        \"\"\"\n",
    "        The BYOL loss, as stated in the BYOL paper (this applies to positive-pair batches x_1, x_2).\n",
    "        Firstly, normalises the embeddings to unit vectors.\n",
    "        Then, computes the dot product between positive-pair embeddings.\n",
    "        The factors of 2 could be removed, but are maintained for consistency with the paper.\n",
    "        \"\"\"\n",
    "        norm_x_1 = F.normalize(x_1, dim = -1, p = 2)\n",
    "        norm_x_2 = F.normalize(x_2, dim = -1, p = 2)\n",
    "        \n",
    "        return 2 - 2 * (norm_x_1 * norm_x_2).sum(dim = -1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Returns the encoding (without projection) of the input, corresponding to the online network.\n",
    "        \"\"\"\n",
    "        if x.shape[:2] != (self.RESNET_DIM, self.RESNET_DIM):\n",
    "            x = self.img_to_resnet(x)\n",
    "            \n",
    "        return self.online_network.encode(x)\n",
    "\n",
    "    def compile_optimiser(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Sets the optimiser parameters.\n",
    "        \"\"\"\n",
    "        self.optimiser = optim.Adam(self.parameters(), **kwargs)\n",
    "    \n",
    "    def get_loss(self, x_1, x_2):\n",
    "        \"\"\"\n",
    "        Computes the loss given positive-pair batches (x_1, x_2)\n",
    "        \"\"\"\n",
    "        \n",
    "        # compute online encodings\n",
    "        online_projection_1 = self.online_network(x_1)\n",
    "        online_projection_2 = self.online_network(x_2)\n",
    "\n",
    "        # compute target encodings\n",
    "        with torch.no_grad():\n",
    "            target_projection_1 = self.target_network(x_1)\n",
    "            target_projection_2 = self.target_network(x_2)\n",
    "\n",
    "        # predict target encodings from online encodings\n",
    "        online_prediction_1 = self.online_predictor(online_projection_1)\n",
    "        online_prediction_2 = self.online_predictor(online_projection_2)\n",
    "\n",
    "        # compute the loss between online-predicted encodings, and target encodings\n",
    "        loss_1 = self.byol_loss(online_prediction_1, target_projection_2.detach())\n",
    "        loss_2 = self.byol_loss(online_prediction_2, target_projection_1.detach())\n",
    "\n",
    "        # average the loss over the batch\n",
    "        return (loss_1 + loss_2).mean()\n",
    "    \n",
    "    def update_checkpoint(self, checkpoint_dir, **checkpoint_data):\n",
    "        \"\"\"\n",
    "        Updates the checkpoint dictionary.\n",
    "        \"\"\"\n",
    "        for k,v in checkpoint_data.items():\n",
    "            if k in self.checkpoint:\n",
    "                self.checkpoint[k] = v\n",
    "\n",
    "        if checkpoint_dir is not None:\n",
    "          torch.save(self.checkpoint, checkpoint_dir)\n",
    "                                                                            \n",
    "    \n",
    "    def train(self, dataloader, epochs, checkpoint_dir = None, transform = None, batch_log_rate = 100):\n",
    "        \"\"\"\n",
    "        Trains the network.\n",
    "        ---------------------------------------------------------------------------------------------------\n",
    "        :param dataloader: a PyTorch DataLoader, containing the training data.\n",
    "        :param epochs: an int, the number of epochs for training.\n",
    "        :param checkpoint_dir: a string, the directory to which to write the checkpoints.\n",
    "        :param transform: a transformation function for the inputs, to apply right before passing it to the \n",
    "                          network. By default no transformation is applied.\n",
    "        :param batch_log_rate: an int. Every batch_log_rate batches, the performance of the network \n",
    "                               is logged. By default logging is performed every 100 batches.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            batch_losses = []\n",
    "            for batch, (x_1,x_2) in enumerate(dataloader):\n",
    "                # x_1 and x_2 are tensors containing patches, \n",
    "                # such that x_1[i] and x_2[i] are patches for the same area\n",
    "                \n",
    "                self.optimiser.zero_grad()\n",
    "\n",
    "                x_1, x_2 = transform(x_1.to(self.device)), transform(x_2.to(self.device))\n",
    "                \n",
    "                loss = self.get_loss(x_1, x_2)\n",
    "\n",
    "                batch_losses.append(loss.cpu())\n",
    "                \n",
    "                loss.backward()\n",
    "                self.optimiser.step()\n",
    "                self.update_target_network()\n",
    "\n",
    "                if batch % (len(dataloader) // 100 + 1) == 0:\n",
    "                    with torch.no_grad():\n",
    "                        avg_loss = np.mean(batch_losses[-20:])\n",
    "                        print(f\"Epoch {epoch + 1}: [{batch + 1}/{len(dataloader)}] ---- BYOL-Loss = {avg_loss}\")\n",
    "                        \n",
    "                        self.update_checkpoint(checkpoint_dir = checkpoint_dir,\n",
    "                                               epoch = epoch,\n",
    "                                              batch = batch,\n",
    "                                              model_state_dict = self.state_dict(),\n",
    "                                              optimiser_state_dict = self.optimiser.state_dict,\n",
    "                                              loss = loss.cpu(),\n",
    "                                              avg_loss_20 = avg_loss,\n",
    "                                              run_end = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            self.update_checkpoint(checkpoint_dir = checkpoint_dir,\n",
    "                                   epoch = epochs,\n",
    "                                   batch = len(dataloader),\n",
    "                                   model_state_dict = self.state_dict(),\n",
    "                                   optimiser_state_dict = self.optimiser.state_dict,\n",
    "                                   loss = loss.cpu(),\n",
    "                                   avg_loss_20 = np.mean(batch_losses[-20:]),\n",
    "                                   run_end = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "        \n",
    "        return self.checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11f19e-96d4-46b5-bfa5-f9023743577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector_parameters = {\"input_dim\" : 2048, \n",
    "                    \"hidden_dim\" : 4096, \n",
    "                    \"output_dim\" : 256, \n",
    "                    \"activation\" : nn.ReLU(), \n",
    "                    \"use_bias\" : True,\n",
    "                    \"use_batch_norm\" : True}\n",
    "\n",
    "predictor_parameters = {\"input_dim\" : 256, \n",
    "                    \"hidden_dim\" : 1024, \n",
    "                    \"output_dim\" : 256, \n",
    "                    \"activation\" : nn.ReLU(), \n",
    "                    \"use_bias\" : True,\n",
    "                    \"use_batch_norm\" : True}\n",
    "\n",
    "encoder = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "byol_nn = MapBYOL(encoder = encoder, \n",
    "                  encoder_layer_idx = -2,\n",
    "                  projector_parameters = projector_parameters, \n",
    "                  predictor_parameters = predictor_parameters, \n",
    "                  ema_tau = 0.99)\n",
    "\n",
    "byol_nn.compile_optimiser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad12b32-547b-4e27-a22d-cce0aefcceba",
   "metadata": {},
   "source": [
    "### W&B BYOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d2bbef-5085-4f92-a5a8-d91dbe81724e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WBMapBYOL(MapBYOL):\n",
    "    \"\"\"\n",
    "    A BYOL model with Weights & Biases integration.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, encoder_layer_idx, projector_parameters, predictor_parameters, ema_tau):\n",
    "        \"\"\"\n",
    "        encoder: a nn.Module, containing an encoder network.\n",
    "        encoder_layer_idx: an int, corresponding to the index of the layer of the encoder \n",
    "                           which is actually used for encoding.\n",
    "                           For instance, in BYOL, if using a ResNet as an encoder, \n",
    "                           the paper uses the output of the last average pooliing layer, \n",
    "                           which is the penultimate layer of the ResNet \n",
    "                           (corresponding to encoder_layer_idx = -2).\n",
    "        projector_parameters: a dict, containing the parameters to initialise an MLP to act as \n",
    "                              a projector network.\n",
    "        projector_parameters: a dict, containing the parameters to initialise an MLP to act as \n",
    "                              a predictor network.\n",
    "        ema_tau: a float (between 0 and 1). The constant used to compute the exponential moving \n",
    "                 average when deriving the target network parameters.\n",
    "        \"\"\"\n",
    "        super(WBMapBYOL, self).__init__(encoder, encoder_layer_idx, projector_parameters, predictor_parameters, ema_tau)\n",
    "        \n",
    "        \n",
    "    def train(self, dataloader, epochs, checkpoint_dir = None, transform = None, batch_log_rate = 100):\n",
    "            \"\"\"\n",
    "            Trains the network.\n",
    "            ---------------------------------------------------------------------------------------------------\n",
    "            :param dataloader: a PyTorch DataLoader, containing the training data.\n",
    "            :param epochs: an int, the number of epochs for training.\n",
    "            :param checkpoint_dir: a string, the directory to which to write the checkpoints.\n",
    "            :param transform: a transformation function for the inputs, to apply right before passing it to the \n",
    "                              network. By default no transformation is applied.\n",
    "            :param batch_log_rate: an int. Every batch_log_rate batches, the performance of the network \n",
    "                                   is logged. By default logging is performed every 100 batches.\n",
    "\n",
    "            \"\"\"\n",
    "            \n",
    "            wandb.init(project=\"honours-project\",\n",
    "                       name = f\"gdrive_experiment_{time.strftime(\"%Y%m%d-%H%M%S\")}\",\n",
    "                       config={\n",
    "                        \"epochs\": epochs,\n",
    "                        \"batch_size\": dataloader.batch_size,\n",
    "                        \"learning_rate\": self.optimiser.param_groups[-1]['lr'],\n",
    "                        \"architecture\": \"BYOL (RESNET Encoder)\"\n",
    "                        })\n",
    "\n",
    "            self.to(self.device)\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                batch_losses = []\n",
    "                for batch, (x_1,x_2) in enumerate(dataloader):\n",
    "                    # x_1 and x_2 are tensors containing patches, \n",
    "                    # such that x_1[i] and x_2[i] are patches for the same area\n",
    "\n",
    "                    self.optimiser.zero_grad()\n",
    "\n",
    "                    x_1, x_2 = transform(x_1.to(self.device)), transform(x_2.to(self.device))\n",
    "\n",
    "                    loss = self.get_loss(x_1, x_2)\n",
    "\n",
    "                    batch_losses.append(loss.cpu())\n",
    "\n",
    "                    loss.backward()\n",
    "                    self.optimiser.step()\n",
    "                    self.update_target_network()\n",
    "\n",
    "                    if batch % (len(dataloader) // 100 + 1) == 0:\n",
    "                        with torch.no_grad():\n",
    "                            avg_loss = np.mean(batch_losses[-20:])\n",
    "                            print(f\"Epoch {epoch + 1}: [{batch + 1}/{len(dataloader)}] ---- BYOL-Loss = {avg_loss}\")\n",
    "\n",
    "                            self.update_checkpoint(checkpoint_dir = checkpoint_dir,\n",
    "                                                   epoch = epoch,\n",
    "                                                    batch = batch,\n",
    "                                                    model_state_dict = self.state_dict(),\n",
    "                                                    optimiser_state_dict = self.optimiser.state_dict,\n",
    "                                                    loss = loss.cpu(),\n",
    "                                                    avg_loss_20 = avg_loss,\n",
    "                                                    run_end = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "                            \n",
    "                            metrics = {\"BYOL/train_loss\": self.checkpoint[\"loss\"], \n",
    "                                       \"BYOL/avg_20_train_loss\": self.checkpoint[\"avg_loss_20\"], \n",
    "                                       \"BYOL/epoch\" : epoch,\n",
    "                                       \"BYOL/batch\" : batch}\n",
    "\n",
    "                            wandb.log(metrics)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.update_checkpoint(checkpoint_dir = checkpoint_dir,\n",
    "                                       epoch = epochs,\n",
    "                                       batch = len(dataloader),\n",
    "                                       model_state_dict = self.state_dict(),\n",
    "                                       optimiser_state_dict = self.optimiser.state_dict,\n",
    "                                       loss = loss.cpu(),\n",
    "                                       avg_loss_20 = np.mean(batch_losses[-20:]),\n",
    "                                       run_end = datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "                \n",
    "                metrics = {\"BYOL/train_loss\": self.checkpoint[\"loss\"], \n",
    "                                       \"BYOL/avg_20_train_loss\": self.checkpoint[\"avg_loss_20\"]}\n",
    "\n",
    "                wandb.log(metrics)\n",
    "                    \n",
    "            wandb.finish()\n",
    "\n",
    "            return self.checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d23ae-f3bb-4cb2-aeac-c97ea74476fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "projector_parameters = {\"input_dim\" : 2048, \n",
    "                    \"hidden_dim\" : 4096, \n",
    "                    \"output_dim\" : 256, \n",
    "                    \"activation\" : nn.ReLU(), \n",
    "                    \"use_bias\" : True,\n",
    "                    \"use_batch_norm\" : True}\n",
    "\n",
    "predictor_parameters = {\"input_dim\" : 256, \n",
    "                    \"hidden_dim\" : 1024, \n",
    "                    \"output_dim\" : 256, \n",
    "                    \"activation\" : nn.ReLU(), \n",
    "                    \"use_bias\" : True,\n",
    "                    \"use_batch_norm\" : True}\n",
    "\n",
    "encoder = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "wb_byol_nn = WBMapBYOL(encoder = encoder, \n",
    "                  encoder_layer_idx = -2,\n",
    "                  projector_parameters = projector_parameters, \n",
    "                  predictor_parameters = predictor_parameters, \n",
    "                  ema_tau = 0.99)\n",
    "\n",
    "wb_byol_nn.compile_optimiser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c31fca-e726-4612-9429-09f2ef7f78ba",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64744fa-cc92-42fa-903c-e6ffc265b30d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Testing Dataset Pairings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c167dff3-734d-43b1-8c79-039637d0fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_patch_pairs(patches):\n",
    "    for patch_1, patch_2 in patches:\n",
    "        fig, ax = plt.subplots(1,2,figsize = (10,5))\n",
    "        ax[0].imshow(patch_1)\n",
    "        ax[1].imshow(patch_2)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a8a21e-d067-42ff-a234-d006616fa5be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cl_patch_dataset = CLPatchDataset.from_dir(\"../src/data/originals\", 64, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab32074-a1d5-4357-aefc-1c8cbdb25c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cl_patch_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d778f00b-3f3e-4aca-9078-db3e67277513",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_patch_loader = DataLoader(cl_patch_dataset, batch_size = 50, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05c5f00-f071-430f-a3b2-5f267c94e859",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_batch = next(iter(cl_patch_loader))\n",
    "patches = [(first_batch[0][i], first_batch[1][i]) for i in range(len(first_batch[0]))]\n",
    "show_patch_pairs(patches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "henv",
   "language": "python",
   "name": "henv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
